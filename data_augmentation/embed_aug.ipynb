{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import re\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.model.word_stats as nmw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_dir):\n",
    "    result = []\n",
    "    with open(file_dir,'r') as f:\n",
    "\t    for line in f:\n",
    "\t\t    result.append(line.strip('\\n'))\n",
    "    return result\n",
    "\n",
    "def read_array(file_dir):\n",
    "    str = []\n",
    "    with open(file_dir,'r') as f:\n",
    "        for line in f:\n",
    "            temp = line.strip('\\n').split(' ')\n",
    "            str.append(np.genfromtxt(np.array(temp)))\n",
    "    return np.array(str)\n",
    "\n",
    "def _tokenizer(text, token_pattern=r\"(?u)\\b\\w\\w+\\b\"):\n",
    "    token_pattern = re.compile(token_pattern)\n",
    "    return token_pattern.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr[:len(arr)-1], dtype='float64')\n",
    "\n",
    "def load_embedding(file):\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def make_embedding_matrix(embedding, tokenizer, len_voc):\n",
    "    all_embs = np.stack(embedding.values())\n",
    "    emb_mean, emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (len_voc, embed_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= len_voc:\n",
    "            continue\n",
    "        embedding_vector = embedding.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenizer(texts, len_voc):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    t = Tokenizer(num_words=len_voc)\n",
    "    t.fit_on_texts(texts)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/Suwako/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3326: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "emb_array = load_embedding('data/bora.emb.vec')\n",
    "len_voc = len(emb_array)\n",
    "\n",
    "train_x = read_file(\"data/bora.train\")\n",
    "tokenizer = make_tokenizer(train_x, len_voc)\n",
    "\n",
    "embed_mat = make_embedding_matrix(emb_array, tokenizer, len_voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "synonyms_number = 5\n",
    "word_number = 2000\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=synonyms_number + 1).fit(embed_mat)\n",
    "neighbours_mat = nn.kneighbors(embed_mat[1:word_number])[1]\n",
    "synonyms = {x[0]: x[1:] for x in neighbours_mat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors_matrix = nn.kneighbors(X = embed_mat[1:word_number], return_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_word = {0: ''}\n",
    "for word in tokenizer.word_index.keys():\n",
    "    index_word[tokenizer.word_index[word]] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "all_distance, all_neighbors = nn.kneighbors(X = embed_mat, return_distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_target = []\n",
    "for i in range(all_distance.shape[0]):\n",
    "    if (all_distance[i][1:].mean() < 40 and len(index_word.get(i)) > 2):\n",
    "        index_target.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "neighbours_target = nn.kneighbors(embed_mat[index_target])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = {x[0]: x[1:] for x in neighbours_target}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify and write sentence\n",
    "\n",
    "def modify_sentence(sentence, synonyms_words, p=0.5):\n",
    "    for i in range(len(sentence)):\n",
    "        if np.random.random() > p:\n",
    "            try:\n",
    "                syns = synonyms_words[sentence[i]]\n",
    "                sentence[i] = np.random.choice(syns)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_words = {}\n",
    "for x in index_target:\n",
    "    try:\n",
    "        temp = [index_word[synonyms[x][i]] for i in range(synonyms_number-1)]\n",
    "        synonyms_words[index_word[x]] = list(filter(lambda x: len(x) > 2, temp))\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish\n"
     ]
    }
   ],
   "source": [
    "with open('output/bora_embed.txt', 'a') as f:\n",
    "    for x in train_x:\n",
    "        modified = modify_sentence(x.split(' '), synonyms_words, p = 0)\n",
    "        sentence_m = ' '.join(modified)\n",
    "        f.write(sentence_m+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
